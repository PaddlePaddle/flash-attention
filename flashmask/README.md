<div align="center">

# âš¡ FlashMask

**Efficient and Rich Mask Extension of FlashAttention**


[![arXiv](https://img.shields.io/badge/arXiv-2410.01359-b31b1b.svg)](https://arxiv.org/abs/2410.01359)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/PaddlePaddle/flash-attention/blob/main/LICENSE)
[![Paddle](https://img.shields.io/badge/Paddle-3.3.0+-orange.svg)](https://www.paddlepaddle.org.cn/)

<p align="center">
  <img src="https://github.com/user-attachments/assets/f2a9cef4-9ad1-49a5-a791-711550ae5957" width="85%" alt="FlashMask Overview"/>
</p>
<p><i>Figure 1: (a) Supported Mask Types, (b) ColumnWise Sparse Representation, (c) Efficient Implementation</i></p>

</div>

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Performance](#-performance)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Citation](#-citation)

---


## ğŸ¯ Overview
We propose FlashMask, an extension of FlashAttention that introduces a **column-wise sparse representation** of attention masks. 

This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, FlashMask achieves **linear memory complexity O(N)**, suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. 

### Core Equation

The core equation utilized in FlashMask is as follows:

$$
\text{result} = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}} + M\right) \cdot V
$$

In this equation:
+ $Q$, $K$, and $V$ are the input tensors to the attention module.
+ All these tensors share the same dimensions.
+ $d$ denotes the size of the last dimension of these tensors.
+ $M$ represents the column-wise sparse mask introduced by FlashMask.

---

## âœ¨ Key Features

<details open>
<summary><b>ğŸ”¢ Version Comparison Matrix</b></summary>

<table align="center">
<thead>
<tr>
<th>Category</th>
<th>Feature</th>
<th align="center">FlashMask</th>
<th align="center">FlashMask V3</th>
<th align="center">FlashMask V4</th>
</tr>
</thead>
<tbody>

<tr>
<td rowspan="7" align="center"><b>Training</b></td>
<td>Custom Mask</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
</tr>
<tr>
<td>Context Parallel</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
</tr>
<tr>
<td>Block Mask</td>
<td align="center">âŒ</td>
<td align="center">âœ…</td>
<td align="center">âŒ</td>
</tr>
<tr>
<td>Support Head Dim</td>
<td align="center">up to 256</td>
<td align="center">up to 256</td>
<td align="center">64, 128</td>
</tr>
<tr>
<td>Data Type</td>
<td align="center">FP16, BF16</td>
<td align="center">BF16</td>
<td align="center">BF16</td>
</tr>
<tr>
<td>Deterministic</td>
<td align="center">âœ…</td>
<td align="center">âœ…*</td>
<td align="center">âœ…</td>
</tr>
<tr>
<td>FP8</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
</tr>

<tr style="border-top: 2px solid #ddd;">
<td rowspan="5" align="center"><b>Inference</b></td>
<td>Custom Mask</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
</tr>
<tr>
<td>PagedAttn</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
</tr>
<tr>
<td>Split KV</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
</tr>
<tr>
<td>PackGQA</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
</tr>
<tr>
<td>FP8</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
<td align="center">âŒ</td>
</tr>

<tr style="border-top: 2px solid #ddd;">
<td rowspan="2" align="center"><b>Framework</b></td>
<td>Paddle</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
</tr>
<tr>
<td>PyTorch</td>
<td align="center">âŒ</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
</tr>

</tbody>
</table>

<small>*V3 deterministic only for head dim â‰¤128</small>

</details>


---

## ğŸš€ Performance

### FlashMask
#### End-to-End Training Speedup
FlashMask achieves **1.65Ã— to 3.22Ã—** end-to-end speedup compared to dense FlashAttention methods across SFT, LoRA, DPO, and RM tasks.

<p align="center">
  <img src="https://github.com/user-attachments/assets/6ca490b4-b57b-4e6a-a913-57517754a47e" width="80%" alt="Training Throughput"/>
</p>

Note: Benchmark data is sourced from the associated research paper.

#### Kernel-Level Benchmarks (A100-SXM 80GB)
FlashMask surpasses **FlexAttention (PyTorch 2.6.0)** by **12.1% to 60.7%** in kernel TFLOPs/s, achieving **37.8% to 62.3%** of theoretical peak FLOPs/s.

<p align="center">
  <img src="https://github.com/user-attachments/assets/2239af39-1787-4338-bfa3-cfc623e88151" width="80%" alt="Kernel Speed Comparison"/>
</p>

Note: Benchmark data is sourced from the associated research paper.

### FlashMask V3 (Hopper Optimized)

| Head Dim | vs FlashMask | vs FlexAttention |
|:--------:|:------------:|:----------------:|
| 128 | +40.2% ~ +141.1% | +7.3% ~ +67.5% |
| 256 | +11.1% ~ +106.2% | +66.9% ~ +212.2% |

Attention-Gym: commit f6ff20

PyTorch: 2.9.0.dev20250901+cu129

<p align="center">
  <img src="https://github.com/user-attachments/assets/cc8c0913-d3a8-4d0d-b6c3-83a299886225" width="85%" alt="V3 Performance"/>
</p>


### Block Mask

FlashMask V3 demonstrates a substantial performance advantage over [Block Attention](https://github.com/mit-han-lab/Block-Sparse-Attention), as shown in the benchmark. Across various sequence lengths (8K, 32K, 128K) and configurations, it achieves a 75.7% to 197.3%â€‹ speedup in forward computation and 48.0% to 94.4% speedup in backward computation.

| Direction | Speedup Range |
|-----------|---------------|
| Forward | **+75.7% ~ +197.3%** |
| Backward | **+48.0% ~ +94.4%** |

<p align="center">
  <img src="https://github.com/user-attachments/assets/bc79b760-9fbe-49d6-a79c-25047904b977" width="48%" alt="Block Mask Fwd"/>
  <img src="https://github.com/user-attachments/assets/613421ad-cdb5-4ad4-b90b-776d6de9f8fc" width="48%" alt="Block Mask Bwd"/>
</p>

---


### MARCO
ğŸ”§ Performance optimizations have been implemented and are currently being pushed upstream.

**MARCO: Mask-Aware Responsive Communication Overlap** eliminates Context Parallel bottlenecks through:

1. **Dynamic Load Balancing** â€” On-the-fly workload estimation for even distribution
2. **Communication Overlapping** â€” Hides KV all-gather latency via computation overlap

#### MARCO vs Magi-Attention (32K & 128K Sequences)

<p align="center">
  <img src="https://github.com/user-attachments/assets/a1dbf4df-6e51-47e8-9c5a-c883dd094166" width="32%" alt="MARCO Bench 1"/>
  <img src="https://github.com/user-attachments/assets/604583f8-c149-4888-ab2a-db8c9ad12985" width="32%" alt="MARCO Bench 2"/>
  <img src="https://github.com/user-attachments/assets/7753cb34-5d1d-4432-89f6-e587b86ffda6" width="32%" alt="MARCO Bench 3"/>
</p>

<details>
<summary><b>ğŸ“Š Detailed Runtime Comparison (ms)</b></summary>
<table>
  <thead>
    <tr>
      <th>Seqlen</th>
      <th>Shape</th>
      <th>Operation</th>
      <th>magi-fwd (ms)</th>
      <th>magi-bwd (ms)</th>
      <th>magi-total (ms)</th>
      <th>FlashMask-fwd (ms)</th>
      <th>FlashMask-bwd (ms)</th>
      <th>FlashMask-total (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="9" align="center" style="vertical-align: middle;"><strong>32K</strong></td>
      <td rowspan="3">b=2, hq=8<br>hkv=1, d=128</td>
      <td>Causal Document Mask</td>
      <td>3.34</td>
      <td>5.22</td>
      <td>8.56</td>
      <td>0.63</td>
      <td>2.07</td>
      <td>2.70</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>3.54</td>
      <td>5.33</td>
      <td>8.88</td>
      <td>0.92</td>
      <td>2.44</td>
      <td>3.37</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>3.39</td>
      <td>5.16</td>
      <td>8.55</td>
      <td>0.63</td>
      <td>2.07</td>
      <td>2.70</td>
    </tr>
    <tr>
      <td rowspan="3">b=2, hq=32<br>hkv=4, d=128</td>
      <td>Causal Document Mask</td>
      <td>3.88</td>
      <td>7.62</td>
      <td>11.50</td>
      <td>1.64</td>
      <td>7.26</td>
      <td>8.90</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>4.88</td>
      <td>11.95</td>
      <td>16.83</td>
      <td>2.70</td>
      <td>8.74</td>
      <td>11.44</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>3.98</td>
      <td>8.05</td>
      <td>12.03</td>
      <td>1.63</td>
      <td>7.26</td>
      <td>8.89</td>
    </tr>
    <tr>
      <td rowspan="3">b=2, hq=64<br>hkv=8, d=128</td>
      <td>Causal Document Mask</td>
      <td>5.07</td>
      <td>13.71</td>
      <td>18.78</td>
      <td>3.00</td>
      <td>14.19</td>
      <td>17.18</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>7.37</td>
      <td>22.30</td>
      <td>29.67</td>
      <td>5.08</td>
      <td>17.15</td>
      <td>22.24</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>5.27</td>
      <td>14.03</td>
      <td>19.30</td>
      <td>2.98</td>
      <td>14.20</td>
      <td>17.18</td>
    </tr>
    <tr>
      <td rowspan="9" align="center" style="vertical-align: middle;"><strong>128K</strong></td>
      <td rowspan="3">b=1, hq=8<br>hkv=1, d=128</td>
      <td>Causal Document Mask</td>
      <td>3.53</td>
      <td>5.65</td>
      <td>9.17</td>
      <td>0.98</td>
      <td>3.57</td>
      <td>4.55</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>3.71</td>
      <td>6.77</td>
      <td>10.48</td>
      <td>1.64</td>
      <td>4.28</td>
      <td>5.92</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>3.55</td>
      <td>5.68</td>
      <td>9.23</td>
      <td>1.08</td>
      <td>3.59</td>
      <td>4.67</td>
    </tr>
    <tr>
      <td rowspan="3">b=1, hq=32<br>hkv=4, d=128</td>
      <td>Causal Document Mask</td>
      <td>4.42</td>
      <td>10.41</td>
      <td>14.83</td>
      <td>2.53</td>
      <td>9.65</td>
      <td>12.18</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>5.50</td>
      <td>15.44</td>
      <td>20.94</td>
      <td>4.69</td>
      <td>13.58</td>
      <td>18.28</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>4.44</td>
      <td>10.53</td>
      <td>14.97</td>
      <td>2.81</td>
      <td>9.85</td>
      <td>12.67</td>
    </tr>
    <tr>
      <td rowspan="3">b=1, hq=64<br>hkv=8, d=128</td>
      <td>Causal Document Mask</td>
      <td>6.18</td>
      <td>19.10</td>
      <td>25.27</td>
      <td>4.66</td>
      <td>18.71</td>
      <td>23.37</td>
    </tr>
    <tr>
      <td>Document Mask</td>
      <td>8.81</td>
      <td>29.39</td>
      <td>38.20</td>
      <td>8.78</td>
      <td>27.18</td>
      <td>35.96</td>
    </tr>
    <tr>
      <td>Prefix LM Document Mask</td>
      <td>6.35</td>
      <td>19.70</td>
      <td>26.05</td>
      <td>5.14</td>
      <td>19.12</td>
      <td>24.26</td>
    </tr>
  </tbody>
</table>
</details>

### FlashMask V4
ğŸ”§ Performance optimizations have been implemented and are currently being pushed upstream.

Baseline: FA4 with mask_mod

Config: Head dim = 128

| Sequence Length | Speedup vs FA4 |
|-----------|---------------|
| 8k | **-0.4% ~ +57.2%** |
| 32k | **-0.7% ~ +39.5%** |
| 128k | **-3.4% ~ +20.9%** |

<p align="center">
  <img src="https://github.com/user-attachments/assets/27085b7b-6e86-44c0-9ec1-6922ea8c2325" width="85%" alt="V3 Performance"/>
</p>

---

## ğŸ“¦ Installation
### ğŸ¥‡ Paddle (Recommended)
#### FlashMask & FlashMask V3
FlashMask and FlashMask V3 are included in the standard PaddlePaddle distribution. No additional plugins are required.
For detailed information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/develop/install/pip/linux-pip.html). 
```bash
python -m pip install paddlepaddle-gpu==3.3.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu129/
```


#### FlashMask V4
```bash
git clone https://github.com/PaddlePaddle/flash-attention.git
cd flash-attention/flashmask
python3 setup.py install
```

#### MARCO
```bash
git clone https://github.com/PaddlePaddle/flash-attention.git
cd flash-attention/csrc/utils/cp_balance/csrc/
python3 setup.py install
```

### PyTorch
#### FlashMask V3
```bash
git clone https://github.com/PaddlePaddle/flash-attention.git
cd flash-attention/csrc/flashmask_v2
python setup.py install
```

#### FlashMask V4
To use the FlashMask V4 features, you need to pull the specific implementation from the PaddlePaddle repository's [Pull Request #103](https://github.com/PaddlePaddle/flash-attention/pull/103). Follow the steps below:
```bash
git clone https://github.com/PaddlePaddle/flash-attention.git
cd flash-attention/flashmask
# Fetch and checkout the specific PR (Pull Request 103)
gh pr checkout 103
python setup.py install
```

## ğŸš€ Quick Start
### How to use FlashMask
#### Installation & Import
```python
from flash_mask.cute.interface import flashmask_attention
```

#### API Reference
```python
def flashmask_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    startend_row_indices: Tensor | None = None,
    *,
    dropout: float = 0.0,
    causal: bool = False,
    window_size: int | tuple | None = None,
    return_softmax_lse: bool = False,
    return_seed_offset: bool = False,
    fixed_seed_offset: Tensor | None = None,
    rng_name: str = "",
    training: bool = True,
    name: str | None = None,
    softmax_scale: float | None = None,
    block_mask: Tensor | None = None,
):
    """

    Args:
        query (Tensor):  The query tensor in the attention module.
            A 4-D tensor with shape [batch_size, q_seq_len, num_heads, head_dim].
            The dtype can be float16 or bfloat16.
        key (Tensor): The key tensor in the attention module.
            A 4-D tensor with shape [batch_size, k_seq_len, k_num_heads, head_dim].
            The dtype can be float16 or bfloat16.
        value (Tensor): The value tensor in the attention module.
            A 4-D tensor with shape [batch_size, k_seq_len, k_num_heads, head_dim].
            The dtype can be float16 or bfloat16.
        startend_row_indices(Tensor):
            A column-wise sparse attention mask row indices tensor.
            A 4-D tensor with shape [batch_size, k_num_heads, k_seq_len, {1, 2, 4}].
            The dtype must be int32. k_num_heads can be 1 or the same as key's num_heads. When num_heads is 1, it will be broadcast to match key's num_heads.
            Depending on the value of the causal parameter, startend_row_indices can take different shapes and meanings.

            - When `causal=True` and the shape is [batch_size, k_num_heads, k_seq_len, 1],
              indicating unidirectional attention. The value represents the starting row index of the left
              lower triangular mask in the dense mask. The value startend_row_indices[..., 0] indicates that elements in the lower left triangle of the attention score matrix starting from the startend_row_indices[..., 0]-th row downwards (inclusive) will be masked.
            - When `causal=True` and the shape is [batch_size, k_num_heads, k_seq_len, 2],
              indicating unidirectional attention. The values represent the starting and ending row indices of
              the left lower triangular mask in the dense mask. The values startend_row_indices[..., 0:2] in startend_row_indices indicate that elements in the lower left triangle of the attention score matrix starting from the startend_row_indices[..., 0]-th row downwards (inclusive) but above the startend_row_indices[..., 1]-th row (exclusive) will be masked.
            - When `causal=False` and the shape is [batch_size, k_num_heads, k_seq_len, 2],
              indicating bidirectional attention. The values represent the starting row index of the left
              lower triangular mask and the ending row index of the right upper triangular mask in the dense mask. The values startend_row_indices[..., 0:2] in startend_row_indices indicate that elements in the lower left triangle of the attention score matrix starting from the startend_row_indices[..., 0]-th row downwards (inclusive) will be masked, and elements in the upper right triangle starting from the startend_row_indices[..., 1]-th row upwards (exclusive) will be masked.
            - When `causal=False` and the shape is [batch_size, k_num_heads, k_seq_len, 4] ,
              indicating bidirectional attention. The values represent the start and end row indices of the
              left lower triangular mask and the start and end row indices of the right upper triangular mask in the dense mask. The values startend_row_indices[..., 0:4] in startend_row_indices indicate that elements in the lower left triangle of the attention score matrix starting from the startend_row_indices[..., 0]-th row downwards (inclusive) but above the startend_row_indices[..., 1] row (exclusive) will be masked, and elements in the upper right triangle starting from the startend_row_indices[..., 2]-th row downwards (inclusive) but above the startend_row_indices[..., 3] row (exclusive) will be masked.

        dropout (float): The dropout ratio. Default is 0.0.
        causal (bool): Whether to enable causal mode. Default is False.
        window_size (int|tuple, optional): Indicates the window size of sliding window local attention.
            If causal mode is enabled, Query at position i will only attend to keys between [i - window_size, i] or [i - window_size[0], i].
            If causal mode is disabled, Query at position i will only attend to keys between [i - window_size, i + window_size] or [i - window_size[0], i + window_size[1]].
        return_softmax_lse (bool): Whether to return the log-sum-exp of the softmax. Default is False.
        return_seed_offset (bool): Whether to return the random seed offset. Default is False.
        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.
        rng_name (str): The name to select Generator.
        training (bool): Whether the module is in training mode. Default is True.
        name (str, optional): Name of the operation. Default is None. Normally, users do not need to set this property.
            For more information, refer to :ref:`api_guide_Name` .
        block_mask (tensor, optional):
            A 4-D integer mask tensor indicating whether each block in the attention matrix should be kept or masked. Must be used together with flashmask.
            The shape should be [batch_size, num_heads, blocklen_q, blocklen_k], where:

            blocklen_q = ceil(seqlen_q / 128), i.e., block_mask.shape[2] must be (seqlen_q + 127) // 128
            blocklen_k = ceil(seqlen_k / 128), i.e., block_mask.shape[3] must be (seqlen_k + 127) // 128
            block_mask.shape[1] (number of heads) must match the num_heads dimension of the flashmask
            Both seqlen_q and seqlen_k must be less than or equal to 128 * 1024
            The dtype should be int32, and each element should be either 0 or 1.
            A value of 1 indicates that the corresponding block is kept (not masked), while 0 means the block is masked.

            Usage Notes:

            Only supported when blockdim_q = blockdim_k = 128 now.
            Only supported when headdim = 128 now.
            This argument must be provided together with flashmask.
            The mask will be applied at the block level: each [i, j] position in block_mask controls whether the corresponding [128 x 128] block in the attention matrix is masked.
            Any mismatch in expected shape or head dimension will raise an error.


    Returns
        Tensor. The computed attention result with the same shape as the input `query`.

    Warning:
        This API only supports inputs with dtype float16 and bfloat16.

    Hint:
        This API supports GQA.
    """
```

#### Implementation Example
Here is a Python example demonstrating how to use the flashmask_attention interface with a block mask implementation.

```python
import pytest
import paddle
from functools import partial
from paddle.nn.functional.flash_attention import flashmask_attention


def generate_sliding_window_mask(batch_size, seqlen_q, seqlen_k, h, window_size = None):
    if window_size == None:
        window_size = 1024
        if seqlen_k != 8192:
            window_size = int(window_size * (seqlen_k / 8192))
            print(f"{seqlen_k=}, auto setting window_size to {window_size}")

    startend_row_indices = paddle.arange(
        window_size, seqlen_k + window_size, dtype="int32"
    ).reshape((1, 1, seqlen_k, 1))
    startend_row_indices = paddle.clip(
        startend_row_indices, max=seqlen_q
    ).repeat_interleave(batch_size, 0)

    causal=True
    return startend_row_indices, causal


# blockmask utils
def random_blockmask(shape, dtype='int32', is_causal=False, ref_q = None):
    mask = paddle.randint(0, 2, shape, dtype=paddle.int32)
    B, S, Q, K = shape
    return mask


def test_flashmask():
    paddle.seed(2024)

    batch_size = 28
    seqlen_q = 128
    seqlen_k = 128
    nheads = 16
    nheads_kv = 4
    nheads_startend_row_indices = 1

    d = 128
    dv = 128

    dtype = paddle.bfloat16

    fa_version = 3

    assert nheads % nheads_kv == 0
    q_ref = paddle.randn(shape=[batch_size, seqlen_q, nheads, d], dtype=dtype)

    k_ref = paddle.randn(shape=[batch_size, seqlen_k, nheads_kv, d], dtype=dtype)
    v_ref = paddle.randn(shape=[batch_size, seqlen_k, nheads_kv, dv], dtype=dtype)

    q_ref.stop_gradient = False
    k_ref.stop_gradient = False
    v_ref.stop_gradient = False

    q_bf16, k_bf16, v_bf16 = [x.detach().clone() for x in (q_ref, k_ref, v_ref)]

    q_bf16.stop_gradient = False
    k_bf16.stop_gradient = False
    v_bf16.stop_gradient = False

    q, k, v = [x.detach().clone() for x in (q_ref, k_ref, v_ref)]

    q.stop_gradient = False
    k.stop_gradient = False
    v.stop_gradient = False

    startend_row_indices, causal = generate_sliding_window_mask(batch_size, seqlen_q, seqlen_k, nheads_startend_row_indices)

    blockmask = random_blockmask(
        shape=[
            startend_row_indices.shape[0],
            startend_row_indices.shape[1],
            (seqlen_q + 127) // 128,
            (seqlen_k + 127) // 128
        ],
        dtype=paddle.int32,
        is_causal=causal,
        ref_q = q_ref
    )

    if fa_version == 2:
        paddle.set_flags({'FLAGS_flash_attn_version': 2})
    elif fa_version == 3:
        paddle.set_flags({'FLAGS_flash_attn_version': 3})
    else:
        raise ValueError(
            f"Invalid flash attention version: {fa_version}"
        )

    out, lse = flashmask_attention(
        q,
        k,
        v,
        startend_row_indices=startend_row_indices,
        causal=causal,
        return_softmax_lse=True,
        block_mask=blockmask
    )
```


### How to use MARCO
#### FlashMask V3 overlapped layer
The following code presents a usable PyLayer object that calls overlapped all-gather automatically under the hood. The users themselves do not need to configure communication overlapping.

```python
importÂ paddle
importÂ paddle.nn.functionalÂ asÂ F
fromÂ paddleÂ importÂ _C_ops
fromÂ paddleÂ importÂ distributedÂ asÂ dist
fromÂ paddle.distributedÂ importÂ fleet
fromÂ paddle.nn.functional.flash_attentionÂ importÂ flashmask_attention
fromÂ paddle.autograd.py_layerÂ importÂ PyLayer

defÂ rearrange_blocks(input_tensor,Â cp_size):
Â Â Â Â #Â runningÂ timeÂ ofÂ theÂ functionÂ isÂ notÂ recorded,Â sinceÂ withÂ workloadÂ balancer
Â Â Â Â #Â weÂ don'tÂ needÂ thisÂ re-arange
Â Â Â Â B,Â _,Â S,Â _Â =Â input_tensor.shape
Â Â Â Â n_blocksÂ =Â cp_sizeÂ *Â 2
Â Â Â Â block_sizeÂ =Â SÂ //Â n_blocks
Â Â Â Â 
Â Â Â Â blocksÂ =Â input_tensor.reshape([B,Â 1,Â n_blocks,Â block_size,Â 2])
Â Â Â Â 
Â Â Â Â indicesÂ =Â []
Â Â Â Â forÂ iÂ inÂ range(cp_size):
Â Â Â Â Â Â Â Â indices.append(i)
Â Â Â Â Â Â Â Â indices.append(n_blocksÂ -Â 1Â -Â i)
Â Â Â Â 
Â Â Â Â reordered_blocksÂ =Â blocks.index_select(index=paddle.to_tensor(indices),Â axis=2)
Â Â Â Â outputÂ =Â reordered_blocks.reshape([B,Â 1,Â S,Â 2])
Â Â Â Â returnÂ output

defÂ reduce_scatter_any_axis_simpler(input_tensor,Â axis,Â group=None):
Â Â Â Â ifÂ groupÂ isÂ None:
Â Â Â Â Â Â Â Â hcgÂ =Â fleet.get_hybrid_communicate_group()
Â Â Â Â Â Â Â Â groupÂ =Â hcg.get_context_parallel_group()

Â Â Â Â parallelismÂ =Â group.nranks
Â Â Â Â ifÂ parallelismÂ ==Â 1:
Â Â Â Â Â Â Â Â returnÂ input_tensor
Â Â Â Â rankÂ =Â group.rank

Â Â Â Â assertÂ input_tensor.shape[axis]Â %Â parallelismÂ ==Â 0,Â (
Â Â Â Â Â Â Â Â f"InputÂ sequenceÂ lengthÂ {input_tensor.shape[axis]}Â can'tÂ beÂ ",
Â Â Â Â Â Â Â Â f"dividedÂ exactlyÂ byÂ contextÂ parallelismÂ '{parallelism}'",
Â Â Â Â )

Â Â Â Â #Â splitÂ intoÂ CP-sizeÂ chunks,Â forÂ example,Â rankÂ 1Â holds:
Â Â Â Â #Â [(1,Â 6),Â (2,Â 5),Â (3,Â 4),Â (0,Â 7)]Â (dual-chunkÂ splitsÂ theÂ KVÂ intoÂ CP-sizeÂ *Â 2Â =Â 8Â smallerÂ chunks)
Â Â Â Â chunksÂ =Â paddle.split(input_tensor,Â parallelism,Â axis=axis)

Â Â Â Â #Â re-arangeÂ chunks:Â theÂ rankÂ 1Â exampleÂ willÂ beÂ [(0,Â 7),Â (1,Â 6),Â (2,Â 5),Â (3,Â 4)]
Â Â Â Â ordered_chunksÂ =Â chunks[-rank:]
Â Â Â Â ordered_chunks.extend(chunks[:-rank])

Â Â Â Â #Â PerformÂ alltoallÂ communication,Â forÂ example,Â rankÂ 2Â gathersÂ allÂ ofÂ theÂ (2,Â 5)Â chunkÂ fromÂ allÂ otherÂ ranks
Â Â Â Â #Â whileÂ itÂ sendsÂ theÂ localÂ (0,Â 7),Â (1,Â 6),Â (3,Â 4)Â toÂ allÂ otherÂ ranks
Â Â Â Â output_buffersÂ =Â [paddle.empty(chunks[0].shape,Â dtype=input_tensor.dtype)Â forÂ _Â inÂ range(parallelism)]
Â Â Â Â dist.stream.alltoall(output_buffers,Â ordered_chunks,Â group=group,Â use_calc_stream=True)

Â Â Â Â #Â SumÂ theÂ receivedÂ chunks
Â Â Â Â resultÂ =Â paddle.stack(output_buffers,Â axis=0).sum(axis=0)
Â Â Â Â returnÂ result


defÂ get_group_unique_id(cg_group,Â rank_to_generate:Â intÂ =Â 0):
Â Â Â Â ifÂ cg_group.rankÂ ==Â rank_to_generate:
Â Â Â Â Â Â Â Â unique_idÂ =Â paddle.nn.functional.flashmask_get_unique_id()
Â Â Â Â else:
Â Â Â Â Â Â Â Â unique_idÂ =Â paddle.zeros([128],Â dtype='uint8',Â device='cpu')
Â Â Â Â result_listÂ =Â []
Â Â Â Â #Â mustÂ useÂ all_gather_object,Â sinceÂ all_gatherÂ (distÂ envÂ isÂ GPUÂ 
Â Â Â Â #Â yetÂ tensorÂ isÂ CPU)Â willÂ throwÂ exception
Â Â Â Â dist.all_gather_object(result_list,Â unique_id,Â group=cg_group)
Â Â Â Â returnÂ result_list[rank_to_generate]

#Â IfÂ weÂ areÂ usingÂ theÂ latestÂ load-balancingÂ strategyÂ developedÂ byÂ haoyang:
#Â sinceÂ theÂ startend_row_indicesÂ areÂ re-orderedÂ [rank0][rank1][rank2][rank3]
#Â andÂ inÂ eachÂ rank,Â theÂ startend_row_indincesÂ areÂ ofÂ theÂ load-balancedÂ state
#Â KVÂ areÂ alsoÂ sorted,Â soÂ weÂ doÂ notÂ needÂ toÂ performÂ dual_chunkÂ andÂ rearrangeÂ 
#Â anymore
defÂ cp_flashmask_overlap_forward(query,Â key,Â value,Â startend_row_indices,Â group,Â causal,Â is_training,Â unique_id:Â paddle.TensorÂ =Â None,Â disable_dual_chunkÂ =Â False):
Â Â Â Â rankÂ =Â group.rank
Â Â Â Â cp_sizeÂ =Â group.world_size

Â Â Â Â #Â All-gatherÂ keyÂ tensorsÂ acrossÂ contextÂ parallelÂ ranks

Â Â Â Â #Â key_gatheredÂ =Â all_gather_balance(key,Â axis=1,Â group=group)
Â Â Â Â #Â value_gatheredÂ =Â all_gather_balance(value,Â axis=1,Â group=group)
Â Â Â Â seq_blocksizeÂ =Â query.shape[1]Â //Â 2

Â Â Â Â #Â PreprocessÂ indicesÂ forÂ dual-chunkÂ strategy
Â Â Â Â ifÂ notÂ disable_dual_chunk:
Â Â Â Â Â Â Â Â startend_row_indicesÂ =Â preprocess_index_dual_chunks(
Â Â Â Â Â Â Â Â Â Â Â Â startend_row_indices,
Â Â Â Â Â Â Â Â Â Â Â Â chunk_id_first=rank,
Â Â Â Â Â Â Â Â Â Â Â Â chunk_id_second=2Â *Â cp_sizeÂ -Â rankÂ -Â 1,
Â Â Â Â Â Â Â Â Â Â Â Â seq_blocksize=seq_blocksize,
Â Â Â Â Â Â Â Â Â Â Â Â max_seqlen_q=seq_blocksize,
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â startend_row_indicesÂ =Â rearrange_blocks(startend_row_indices,Â cp_size)
Â Â Â Â #Â cyclicÂ leftÂ moveÂ onÂ seqlenÂ dim,Â soÂ thatÂ theÂ maskÂ indicesÂ areÂ shiftedÂ correctly
Â Â Â Â processed_maskÂ =Â paddle._C_ops.roll(startend_row_indices,Â shifts=-query.shape[1]Â *Â (rankÂ +Â 1),Â axis=2)
Â Â Â Â 
Â Â Â Â #Â usingÂ localÂ key/valueÂ chunks,Â andÂ overlapÂ theÂ sparseÂ gatherÂ 
Â Â Â Â #Â print(f"StartÂ overlapÂ flashmaskÂ attention:Â {rank}Â /Â {cp_size}")
Â Â Â Â output,Â log_sum_expÂ =Â flashmask_attention(
Â Â Â Â Â Â Â Â query,
Â Â Â Â Â Â Â Â key,
Â Â Â Â Â Â Â Â value,
Â Â Â Â Â Â Â Â startend_row_indices=processed_mask,
Â Â Â Â Â Â Â Â causal=causal,
Â Â Â Â Â Â Â Â return_softmax_lse=True,
Â Â Â Â Â Â Â Â training=is_training,
Â Â Â Â Â Â Â Â unique_id=unique_id,
Â Â Â Â Â Â Â Â rank=rank,
Â Â Â Â Â Â Â Â nranks=cp_size,
Â Â Â Â )
Â Â Â Â returnÂ output,Â log_sum_exp,Â startend_row_indices


defÂ cp_flashmask_overlap_backward(
Â Â Â Â query,Â key,Â value,Â startend_row_indices,Â output,Â log_sum_exp,Â output_grad,Â group,Â causal
):
Â Â Â Â """
Â Â Â Â BackwardÂ passÂ ofÂ contextÂ parallelÂ flashmaskÂ attentionÂ withÂ distributedÂ overlap.

Â Â Â Â ThisÂ functionÂ implementsÂ theÂ backwardÂ passÂ ofÂ flashmaskÂ attentionÂ withÂ contextÂ parallelism,
Â Â Â Â computingÂ gradientsÂ forÂ query,Â key,Â andÂ valueÂ tensors.

Â Â Â Â Args:
Â Â Â Â Â Â Â Â queryÂ (paddle.Tensor):Â QueryÂ tensor
Â Â Â Â Â Â Â Â keyÂ (paddle.Tensor):Â KeyÂ tensor
Â Â Â Â Â Â Â Â valueÂ (paddle.Tensor):Â ValueÂ tensor
Â Â Â Â Â Â Â Â startend_row_indicesÂ (paddle.Tensor):Â ProcessedÂ startend_row_indices
Â Â Â Â Â Â Â Â outputÂ (paddle.Tensor):Â ForwardÂ passÂ output
Â Â Â Â Â Â Â Â log_sum_expÂ (paddle.Tensor):Â Log-sum-expÂ fromÂ forwardÂ pass
Â Â Â Â Â Â Â Â output_gradÂ (paddle.Tensor):Â GradientÂ ofÂ output
Â Â Â Â Â Â Â Â groupÂ (paddle.distributed.Group):Â CommunicationÂ group
Â Â Â Â Â Â Â Â causalÂ (bool):Â WhetherÂ causalÂ attentionÂ wasÂ used

Â Â Â Â Returns:
Â Â Â Â Â Â Â Â tuple:Â (query_grad,Â key_grad,Â value_grad)
Â Â Â Â """
Â Â Â Â rankÂ =Â group.rank
Â Â Â Â cp_sizeÂ =Â group.world_size
Â Â Â Â #Â rollÂ theÂ localÂ chunkÂ toÂ theÂ firstÂ chunkÂ positionÂ (whileÂ fwdÂ rollÂ theÂ localÂ chunkÂ toÂ theÂ back)
Â Â Â Â startend_row_indicesÂ =Â paddle._C_ops.roll(startend_row_indices,Â shifts=-query.shape[1]Â *Â rank,Â axis=2)

Â Â Â Â query_grad,Â key_grad_gathered,Â value_grad_gatheredÂ =Â paddle._C_ops.flashmask_attention_v2_grad(
Â Â Â Â Â Â Â Â query,
Â Â Â Â Â Â Â Â key,
Â Â Â Â Â Â Â Â value,
Â Â Â Â Â Â Â Â output,
Â Â Â Â Â Â Â Â log_sum_exp,
Â Â Â Â Â Â Â Â startend_row_indices,
Â Â Â Â Â Â Â Â None,Â Â #Â block_mask
Â Â Â Â Â Â Â Â output_grad,
Â Â Â Â Â Â Â Â query.shape[-1]Â **Â (-0.5),
Â Â Â Â Â Â Â Â False,
Â Â Â Â Â Â Â Â rank,
Â Â Â Â Â Â Â Â cp_size,Â Â Â Â Â Â Â Â Â Â Â Â #Â nranks
Â Â Â Â )

Â Â Â Â key_gradÂ =Â reduce_scatter_any_axis_simpler(key_grad_gathered,Â axis=1,Â group=group)
Â Â Â Â value_gradÂ =Â reduce_scatter_any_axis_simpler(value_grad_gathered,Â axis=1,Â group=group)
Â Â Â Â returnÂ query_grad,Â key_grad,Â value_grad

classÂ OverlappedFlashMask(PyLayer):
Â Â Â Â """
Â Â Â Â CPÂ FlashMaskÂ attentionÂ withÂ NVSHMEM-basedÂ overlapping
Â Â Â Â TheÂ fetchÂ timeÂ ofÂ requiredÂ KVÂ chunksÂ areÂ overlappedÂ (hidden),Â andÂ theÂ kernel
Â Â Â Â willÂ treatÂ theÂ attentionÂ calculationÂ asÂ usingÂ theÂ entireÂ KVÂ tensorÂ (localÂ sizeÂ *Â cp_size)
Â Â Â Â soÂ thatÂ weÂ don'tÂ haveÂ throughputÂ loss
Â Â Â Â """

Â Â Â Â is_first_callÂ =Â True

Â Â Â Â @staticmethod
Â Â Â Â defÂ forward(
Â Â Â Â Â Â Â Â ctx,
Â Â Â Â Â Â Â Â query,
Â Â Â Â Â Â Â Â key,
Â Â Â Â Â Â Â Â value,
Â Â Â Â Â Â Â Â startend_row_indices,
Â Â Â Â Â Â Â Â fixed_seed_offset=None,
Â Â Â Â Â Â Â Â dropout=0.0,
Â Â Â Â Â Â Â Â causal=False,
Â Â Â Â Â Â Â Â training=True,
Â Â Â Â Â Â Â Â group=None,
Â Â Â Â Â Â Â Â mode="overlap",
Â Â Â Â ):
Â Â Â Â Â Â Â Â """
Â Â Â Â Â Â Â Â ForwardÂ passÂ ofÂ FlashMaskÂ attentionÂ withÂ contextÂ parallelism.

Â Â Â Â Â Â Â Â Args:
Â Â Â Â Â Â Â Â Â Â Â Â ctx:Â ContextÂ objectÂ forÂ savingÂ informationÂ forÂ backwardÂ pass
Â Â Â Â Â Â Â Â Â Â Â Â queryÂ (paddle.Tensor):Â QueryÂ tensor,Â pre-dividedÂ byÂ CPÂ size
Â Â Â Â Â Â Â Â Â Â Â Â keyÂ (paddle.Tensor):Â KeyÂ tensor,Â pre-dividedÂ byÂ CPÂ size
Â Â Â Â Â Â Â Â Â Â Â Â valueÂ (paddle.Tensor):Â ValueÂ tensor,Â pre-dividedÂ byÂ CPÂ size
Â Â Â Â Â Â Â Â Â Â Â Â startend_row_indicesÂ (paddle.Tensor):Â RowÂ indicesÂ forÂ attentionÂ mask
Â Â Â Â Â Â Â Â Â Â Â Â fixed_seed_offsetÂ (paddle.Tensor,Â optional):Â FixedÂ seedÂ offsetÂ forÂ dropout
Â Â Â Â Â Â Â Â Â Â Â Â dropoutÂ (float):Â DropoutÂ probability
Â Â Â Â Â Â Â Â Â Â Â Â causalÂ (bool):Â WhetherÂ toÂ useÂ causalÂ attention
Â Â Â Â Â Â Â Â Â Â Â Â trainingÂ (bool):Â WhetherÂ inÂ trainingÂ mode
Â Â Â Â Â Â Â Â Â Â Â Â modeÂ (str):Â AttentionÂ mode,Â currentlyÂ supportsÂ "allgather_kv"

Â Â Â Â Â Â Â Â Returns:
Â Â Â Â Â Â Â Â Â Â Â Â paddle.Tensor:Â AttentionÂ output

Â Â Â Â Â Â Â Â Raises:
Â Â Â Â Â Â Â Â Â Â Â Â NotImplementedError:Â IfÂ dropoutÂ >Â 0.0Â orÂ causal=True
Â Â Â Â Â Â Â Â Â Â Â Â AssertionError:Â IfÂ queryÂ sequenceÂ lengthÂ isÂ notÂ divisibleÂ byÂ 2
Â Â Â Â Â Â Â Â """
Â Â Â Â Â Â Â Â #Â ValidateÂ inputÂ parameters
Â Â Â Â Â Â Â Â ifÂ dropoutÂ >Â 0.0:
Â Â Â Â Â Â Â Â Â Â Â Â raiseÂ NotImplementedError("DropoutÂ isÂ notÂ supportedÂ inÂ FlashMaskÂ contextÂ parallelÂ yet.")

Â Â Â Â Â Â Â Â ifÂ causal:
Â Â Â Â Â Â Â Â Â Â Â Â raiseÂ NotImplementedError("FlashMaskContextParallelÂ doesÂ notÂ supportÂ causal=TrueÂ yet.")

Â Â Â Â Â Â Â Â ifÂ fixed_seed_offsetÂ isÂ notÂ None:
Â Â Â Â Â Â Â Â Â Â Â Â raiseÂ NotImplementedError("FixedÂ seedÂ offsetÂ isÂ notÂ supportedÂ yet.")

Â Â Â Â Â Â Â Â ifÂ groupÂ isÂ None:
Â Â Â Â Â Â Â Â Â Â Â Â groupÂ =Â dist.fleet.get_hybrid_communicate_group().get_context_parallel_group()

Â Â Â Â Â Â Â Â #Â ValidateÂ queryÂ sequenceÂ lengthÂ forÂ DualChunkSwapÂ strategy
Â Â Â Â Â Â Â Â assertÂ query.shape[1]Â %Â 2Â ==Â 0,Â (
Â Â Â Â Â Â Â Â Â Â Â Â f"QueryÂ sequenceÂ lengthÂ mustÂ beÂ divisibleÂ byÂ 2.Â "
Â Â Â Â Â Â Â Â Â Â Â Â f"FlashMaskContextParallelÂ usesÂ DualChunkSwapÂ strategyÂ forÂ loadÂ balancing.Â "
Â Â Â Â Â Â Â Â Â Â Â Â f"CurrentÂ queryÂ sequenceÂ length:Â {query.shape[1]}"
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â assertÂ key.shape[2]Â <=Â 4,Â (
Â Â Â Â Â Â Â Â Â Â Â Â "KVÂ headÂ expectedÂ toÂ beÂ <=Â 4,Â sinceÂ largeÂ KVÂ headÂ increasesÂ communicationÂ load,"
Â Â Â Â Â Â Â Â Â Â Â Â f"WhichÂ mightÂ notÂ beÂ suitableÂ forÂ overlapping.Â CurrentÂ KVÂ numÂ head:Â {key.shape[2]}"
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â assertÂ startend_row_indices.shape[1]Â ==Â 1,Â (
Â Â Â Â Â Â Â Â Â Â Â Â f"Currently,Â weÂ onlyÂ supportÂ maskÂ numÂ headÂ =Â 1,Â butÂ got:Â {startend_row_indices.shape[1]}"
Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â unique_id_tensorÂ =Â None
Â Â Â Â Â Â Â Â ifÂ OverlappedFlashMask.is_first_call:
Â Â Â Â Â Â Â Â Â Â Â Â OverlappedFlashMask.is_first_callÂ =Â False
Â Â Â Â Â Â Â Â Â Â Â Â unique_id_tensorÂ =Â get_group_unique_id(group)

Â Â Â Â Â Â Â Â #Â PerformÂ forwardÂ pass
Â Â Â Â Â Â Â Â output,Â log_sum_exp,Â startend_row_indicesÂ =Â cp_flashmask_overlap_forward(
Â Â Â Â Â Â Â Â Â Â Â Â query,Â key,Â value,Â startend_row_indices,Â group,Â causal,Â training,Â unique_id_tensor,
Â Â Â Â Â Â Â Â Â Â Â Â disable_dual_chunkÂ =Â modeÂ ==Â "balance_overlap"
Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â #Â SaveÂ tensorsÂ forÂ backwardÂ pass
Â Â Â Â Â Â Â Â ctx.save_for_backward(query,Â key,Â value,Â output,Â log_sum_exp,Â startend_row_indices)
Â Â Â Â Â Â Â Â ctx.groupÂ =Â group
Â Â Â Â Â Â Â Â ctx.causalÂ =Â causal

Â Â Â Â Â Â Â Â returnÂ output

Â Â Â Â @staticmethod
Â Â Â Â defÂ backward(ctx,Â output_grad):
Â Â Â Â Â Â Â Â """
Â Â Â Â Â Â Â Â BackwardÂ passÂ ofÂ FlashMaskÂ attentionÂ withÂ contextÂ parallelism.

Â Â Â Â Â Â Â Â Args:
Â Â Â Â Â Â Â Â Â Â Â Â ctx:Â ContextÂ objectÂ withÂ savedÂ information
Â Â Â Â Â Â Â Â Â Â Â Â output_gradÂ (paddle.Tensor):Â GradientÂ ofÂ output

Â Â Â Â Â Â Â Â Returns:
Â Â Â Â Â Â Â Â Â Â Â Â tuple:Â GradientsÂ forÂ allÂ inputÂ arguments
Â Â Â Â Â Â Â Â """
Â Â Â Â Â Â Â Â #Â RetrieveÂ savedÂ tensors
Â Â Â Â Â Â Â Â query,Â key,Â value,Â output,Â log_sum_exp,Â startend_row_indicesÂ =Â ctx.saved_tensor()
Â Â Â Â Â Â Â Â groupÂ =Â ctx.group
Â Â Â Â Â Â Â Â causalÂ =Â ctx.causal

Â Â Â Â Â Â Â Â #Â ComputeÂ gradients
Â Â Â Â Â Â Â Â query_grad,Â key_grad,Â value_gradÂ =Â cp_flashmask_overlap_backward(
Â Â Â Â Â Â Â Â Â Â Â Â query,Â key,Â value,Â startend_row_indices,Â output,Â log_sum_exp,Â output_grad,Â group,Â causal
Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â returnÂ query_grad,Â key_grad,Â value_grad
```

#### FlashMask V3 using load balancing
Since load-balancing module works on sharding Q, K, V and rearrange mask tensors, the module can be used independently with overlap attention layer. Therefore, users are free to choose whether some of the functionalities should be switched off. The following code presents an example for using load balancing:

```python
importÂ paddle
fromÂ paddle.distributedÂ importÂ fleet
fromÂ cp_balance.cp_balanceÂ importÂ balance_flashmask_input

fleet.init(is_collective=True,Â strategy={
Â Â Â Â "your_strategy":Â "thisÂ isÂ aÂ dummyÂ one"
})
cp_groupÂ =Â fleet.get_hybrid_communicate_group().get_context_parallel_group()

cp_sizeÂ =Â cp_group.world_size
cp_rankÂ =Â cp_group.rank
local_mask,Â bucketsÂ =Â balance_flashmask_input(startend_row_indices.clone(),Â cp_size,Â cp_rank)
#Â getÂ localÂ shardedÂ QKV

full_q,Â full_k,Â full_vÂ =Â get_full_qkv_from_some_source()

defÂ scatter_balance(input_tensor,Â group=None,Â axis=0,Â buckets=None):
Â Â Â Â """
Â Â Â Â EvenlyÂ splitÂ inputÂ tensorÂ alongÂ theÂ specifiedÂ axisÂ acrossÂ modelÂ parallelÂ ranks.

Â Â Â Â ThisÂ functionÂ implementsÂ balancedÂ scatteringÂ byÂ takingÂ chunksÂ fromÂ bothÂ ends
Â Â Â Â ofÂ theÂ tensorÂ toÂ ensureÂ loadÂ balancingÂ acrossÂ ranks.

Â Â Â Â Args:
Â Â Â Â Â Â Â Â input_tensorÂ (paddle.Tensor):Â InputÂ tensorÂ toÂ beÂ scattered
Â Â Â Â Â Â Â Â groupÂ (paddle.distributed.Group,Â optional):Â CommunicationÂ group.
Â Â Â Â Â Â Â Â Â Â Â Â IfÂ None,Â usesÂ modelÂ parallelÂ groupÂ fromÂ fleet
Â Â Â Â Â Â Â Â axisÂ (int,Â optional):Â AxisÂ alongÂ whichÂ toÂ scatter.Â DefaultsÂ toÂ 0

Â Â Â Â Returns:
Â Â Â Â Â Â Â Â paddle.Tensor:Â ScatteredÂ tensorÂ chunkÂ forÂ currentÂ rank

Â Â Â Â Note:
Â Â Â Â Â Â Â Â ThisÂ APIÂ isÂ differentÂ fromÂ distributed.scatterÂ -Â itÂ performsÂ balanced
Â Â Â Â Â Â Â Â splittingÂ byÂ takingÂ chunksÂ fromÂ bothÂ endsÂ ofÂ theÂ sequence.
Â Â Â Â """
Â Â Â Â ifÂ groupÂ isÂ None:
Â Â Â Â Â Â Â Â hcgÂ =Â fleet.get_hybrid_communicate_group()
Â Â Â Â Â Â Â Â groupÂ =Â hcg.get_model_parallel_group()

Â Â Â Â parallelismÂ =Â group.nranks
Â Â Â Â ifÂ parallelismÂ ==Â 1:
Â Â Â Â Â Â Â Â returnÂ input_tensor.clone()

Â Â Â Â rankÂ =Â group.rank
Â Â Â Â seq_lenÂ =Â input_tensor.shape[axis]

Â Â Â Â assertÂ len(buckets)Â ==Â parallelism,Â "bucketsÂ shouldÂ haveÂ sameÂ sizeÂ asÂ parallelism"
Â Â Â Â assertÂ seq_lenÂ %Â (parallelismÂ *Â len(buckets[rank]))Â ==Â 0,Â "seq_lenÂ mustÂ beÂ divisibleÂ byÂ parallelismÂ *Â len(buckets[rank])"
Â Â Â Â local_chunksÂ =Â []
Â Â Â Â balance_chunksizeÂ =Â seq_lenÂ //Â (parallelismÂ *Â len(buckets[rank]))
Â Â Â Â for(_,Â idx)Â inÂ buckets[rank]:
Â Â Â Â Â Â Â Â chunk_startÂ =Â idxÂ *Â balance_chunksize
Â Â Â Â Â Â Â Â chunk_endÂ =Â (idxÂ +Â 1)Â *Â balance_chunksize
Â Â Â Â Â Â Â Â chunkÂ =Â paddle.slice(input_tensor,Â axes=[axis],Â starts=chunk_start,Â ends=chunk_end)
Â Â Â Â Â Â Â Â local_chunks.append(chunk)
Â Â Â Â returnÂ paddle.concat(local_chunks,Â axis=axis)

qÂ =Â scatter_balance(full_q,Â groupÂ =Â cp_group,Â axis=1,Â bucketsÂ =Â buckets).contiguous()
kÂ =Â scatter_balance(full_k,Â groupÂ =Â cp_group,Â axis=1,Â bucketsÂ =Â buckets).contiguous()
vÂ =Â scatter_balance(full_v,Â groupÂ =Â cp_group,Â axis=1,Â bucketsÂ =Â buckets).contiguous()
```


## ğŸ“ Citation
If you use FlashMask in your research or project, we appreciate that you use the following citations:

```bibtex
@article{wang2024flashmask,
  title={Flashmask: Efficient and rich mask extension of flashattention},
  author={Wang, Guoxia and Zeng, Jinle and Xiao, Xiyuan and Wu, Siming and Yang, Jiabin and Zheng, Lujing and Chen, Zeyu and Bian, Jiang and Yu, Dianhai and Wang, Haifeng},
  journal={arXiv preprint arXiv:2410.01359},
  year={2024}
}
```

